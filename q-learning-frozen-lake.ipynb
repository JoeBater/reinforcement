{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import time\n",
    "import imageio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
    "#env = gym.make('FrozenLake-v1', desc=desc, is_slippery=False)\n",
    "\n",
    "desc8x8 = [\"SFFFFFFF\", \"FFFFFFFF\", \"FFFHFFFF\", \"FFFFFHFF\", \"FFFHFFFF\", \"FHHFFFHF\", \"FHFFHFHF\", \"FFFHFFFG\"]\n",
    "env = gym.make('FrozenLake8x8-v1', desc=desc8x8, is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Discrete(64)\n",
      "Sample observation 36\n"
     ]
    }
   ],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Sample observation\", env.observation_space.sample())  # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space Sample 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample())  # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  64  possible states\n",
      "There are  4  possible actions\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = np.zeros((state_space, action_space))\n",
    "    return Qtable\n",
    "\n",
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)\n",
    "Qtable_frozenlake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "    # Exploitation: take the action with the highest state, action value\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "\n",
    "    return action\n",
    "\n",
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    # Randomly generate a number between 0 and 1\n",
    "    random_num = random.uniform(0, 1)\n",
    "    # if random_num > greater than epsilon --> exploitation\n",
    "    if random_num > epsilon:\n",
    "        # Take the action with the highest value given a state\n",
    "        # np.argmax can be useful here\n",
    "        action = greedy_policy(Qtable, state)\n",
    "    # else --> exploration\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N table for Thompson sampling\n",
    "def initialize_N_table(state_space, action_space):\n",
    "    N = np.zeros((state_space, action_space))\n",
    "    return N\n",
    "\n",
    "def modify_N_table(N, state, action):\n",
    "    N[state][action] += 1\n",
    "\n",
    "N = initialize_N_table(state_space, action_space)\n",
    "    \n",
    "# alpha table for Thompson sampling\n",
    "def initialize_alpha_table(state_space, action_space):\n",
    "    alpha = np.ones((state_space, action_space))\n",
    "    return alpha\n",
    "\n",
    "def modify_alpha_table(alpha, state, action):\n",
    "    alpha[state][action] += 1\n",
    "\n",
    "alpha = initialize_alpha_table(state_space, action_space)\n",
    "\n",
    "# Thompson sampling action selection\n",
    "def thompson_sampling(Qtable, state, N, alpha):\n",
    "    # Generate a random number from a beta distribution\n",
    "    beta_dist = np.random.beta(Qtable[state][:] + 1, N[state][:] - Qtable[state][:] + 1)\n",
    "    # Take the action with the highest beta value\n",
    "    action = np.argmax(beta_dist)\n",
    "\n",
    "    modify_N_table(N, state, action)\n",
    "    modify_alpha_table(alpha, state, action)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 10000  # Total training episodes\n",
    "learning_rate = 0.2  # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 3  # Total number of test episodes\n",
    "\n",
    "# Environment parameters\n",
    "max_steps = 500  # Max steps per episode\n",
    "gamma = 0.9  # Discounting rate\n",
    "eval_seed = []  # The evaluation seed of the environment\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0  # Exploration probability at start\n",
    "min_epsilon = 0.05  # Minimum exploration probability\n",
    "decay_rate = 0.0005  # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "\n",
    "    for episode in range(n_training_episodes):\n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        # Reset the environment\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # repeat\n",
    "        for step in range(max_steps):\n",
    "            # Choose the action At using epsilon greedy policy\n",
    "            #action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "            action = thompson_sampling(Qtable, state, N, alpha)\n",
    "\n",
    "\n",
    "            # Take action At and observe Rt+1 and St+1\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            Qtable[state][action] = Qtable[state][action] + learning_rate * (\n",
    "                reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action]\n",
    "            )\n",
    "\n",
    "            # If terminated or truncated finish the episode\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            # Our next state is the new state\n",
    "            state = new_state\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.27528587e-02, 2.55020090e-02, 2.56161744e-02, 2.66497073e-02],\n",
       "       [2.65855901e-02, 2.77777243e-02, 2.95655385e-02, 3.22339669e-02],\n",
       "       [3.59115534e-02, 4.15010252e-02, 4.12807873e-02, 3.86924475e-02],\n",
       "       [4.69635096e-02, 5.39584566e-02, 5.08802960e-02, 4.53132707e-02],\n",
       "       [5.87556688e-02, 6.55437945e-02, 7.04444620e-02, 6.67076802e-02],\n",
       "       [7.68404248e-02, 7.30497686e-02, 8.74224605e-02, 8.17001334e-02],\n",
       "       [1.01498646e-01, 9.45561576e-02, 1.17180608e-01, 9.55253658e-02],\n",
       "       [1.09765092e-01, 1.26419020e-01, 1.23216301e-01, 1.06154819e-01],\n",
       "       [2.30630939e-02, 2.19420277e-02, 2.26169521e-02, 2.39321147e-02],\n",
       "       [2.53337696e-02, 2.86268747e-02, 2.72982432e-02, 2.74487470e-02],\n",
       "       [3.38046791e-02, 3.32011519e-02, 3.95412642e-02, 4.02986748e-02],\n",
       "       [2.90823199e-02, 1.98525558e-02, 4.03653058e-02, 4.89356642e-02],\n",
       "       [5.88350842e-02, 6.09059360e-02, 6.79233749e-02, 7.53403364e-02],\n",
       "       [8.69428741e-02, 9.49633790e-02, 9.38920603e-02, 8.61319588e-02],\n",
       "       [1.18536638e-01, 1.37513172e-01, 1.26372751e-01, 1.04600766e-01],\n",
       "       [1.40555483e-01, 1.43519327e-01, 1.41016942e-01, 1.18855807e-01],\n",
       "       [1.85198514e-02, 1.66256396e-02, 1.74440680e-02, 2.11090209e-02],\n",
       "       [2.21044644e-02, 1.91068511e-02, 2.13586195e-02, 2.22586294e-02],\n",
       "       [2.22570069e-02, 1.33192108e-02, 2.43788657e-02, 2.09736170e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.88960904e-02, 5.27605754e-02, 7.20916719e-02, 6.45846641e-02],\n",
       "       [3.82109710e-02, 6.60623724e-02, 6.64868193e-02, 1.24967578e-01],\n",
       "       [1.27581964e-01, 1.54641738e-01, 1.77472598e-01, 1.36635810e-01],\n",
       "       [1.61432586e-01, 1.84386670e-01, 1.91037040e-01, 1.63065158e-01],\n",
       "       [1.13049881e-02, 1.30737518e-02, 1.53327476e-02, 1.59990087e-02],\n",
       "       [1.61474127e-02, 1.22429944e-02, 1.37149326e-02, 1.75408246e-02],\n",
       "       [1.30496671e-02, 1.57168008e-02, 1.61633688e-02, 1.96053237e-02],\n",
       "       [7.90676430e-03, 1.28228254e-02, 1.16980194e-02, 2.44927174e-02],\n",
       "       [4.14438356e-02, 2.29444331e-02, 3.98403036e-02, 3.62606142e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.73597401e-01, 1.37201617e-01, 1.92682286e-01, 1.12104762e-01],\n",
       "       [2.41667578e-01, 2.41492220e-01, 2.61772372e-01, 1.98747657e-01],\n",
       "       [9.71797625e-03, 8.10935935e-03, 7.97375696e-03, 9.48699945e-03],\n",
       "       [8.65929570e-03, 4.99885230e-03, 8.35296395e-03, 1.09472192e-02],\n",
       "       [1.44067117e-02, 2.83530416e-03, 3.79899007e-03, 7.26313436e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.54727111e-02, 7.41000158e-02, 4.86821302e-02, 6.97026008e-02],\n",
       "       [4.28624453e-02, 1.07441565e-01, 1.63141491e-01, 9.94496717e-02],\n",
       "       [1.03211232e-01, 1.89018936e-01, 1.49169843e-01, 2.20326628e-01],\n",
       "       [3.29392194e-01, 2.66129439e-01, 3.31683765e-01, 2.48285834e-01],\n",
       "       [5.24944085e-03, 3.63977468e-03, 4.73249018e-03, 2.61489937e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.18254041e-03, 7.08480146e-03, 5.98501226e-03, 1.23440898e-02],\n",
       "       [2.61202622e-02, 2.35809751e-02, 3.89980085e-02, 3.46081280e-02],\n",
       "       [7.43248541e-02, 1.83037333e-02, 6.46161334e-02, 6.63139778e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.15855757e-01, 2.44014368e-01, 4.40958715e-01, 2.04555709e-01],\n",
       "       [3.68857946e-03, 2.24800809e-03, 2.37726419e-03, 2.65206933e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.64236935e-04, 7.94332564e-04, 4.89115760e-04, 5.54509041e-04],\n",
       "       [3.83062459e-03, 2.08881855e-05, 4.18409641e-03, 8.73140404e-05],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.89772599e-02, 6.89233557e-03, 6.08464760e-03, 1.68521117e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.18563341e-01, 3.69872791e-01, 7.25201921e-01, 3.31256206e-01],\n",
       "       [2.60414499e-03, 2.10499154e-03, 2.29600318e-03, 1.94644169e-03],\n",
       "       [1.17166570e-03, 1.30975849e-03, 9.11451734e-04, 1.77281874e-03],\n",
       "       [9.60098068e-04, 7.93392940e-04, 2.25708175e-04, 5.54556856e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.28927432e-03],\n",
       "       [6.53599065e-03, 5.76000000e-02, 2.66411853e-02, 1.08499652e-03],\n",
       "       [6.53599065e-03, 2.00000000e-01, 0.00000000e+00, 1.60000000e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)\n",
    "\n",
    "#print(np.sum(Qtable_frozenlake))\n",
    "Qtable_frozenlake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_reward=0.33 +/- 0.47\n"
     ]
    }
   ],
   "source": [
    "# 8x8 Frozen Lake doesn't always learn anything, so need to re-train possibly multiple times.\n",
    "\n",
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "    \"\"\"\n",
    "    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "    :param env: The evaluation environment\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param Q: The Q-table\n",
    "    :param seed: The evaluation seed array (for taxi-v3)\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    for episode in range(n_eval_episodes):\n",
    "        if seed:\n",
    "            state, info = env.reset(seed=seed[episode])\n",
    "        else:\n",
    "            state, info = env.reset()\n",
    "        step = 0\n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "\n",
    "            # Take the action (index) that have the maximum expected future reward given that state\n",
    "            action = greedy_policy(Q, state)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            \n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
